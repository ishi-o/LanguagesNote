+++

[toc]

+++

# 机器学习

## 基本概念

- 机器学习(`ML`)是实现人工智能的一种方法
- 按结构特点可分为**传统机器学习**和**深度学习**(`DL`)
- 按算法特点可分为无监督学习、监督学习、半监督学习、强化学习
- **监督学习**指样本集被人工贴上标签(`label`)，在模型得出预测值后和标签比较并反过来调整模型，即为有监督的学习过程
  监督学习常用于分类、回归任务
  - **分类任务**：输出为一系列离散的值(不同的类别，可编号)，目标是预测某个**离散型变量**的分布
  - **回归任务**：输出为一系列确切的数值，目标是预测某个**连续型变量**的分布
- **无监督学习**指样本集没有提前贴上标签，完全靠分析(寻找隐藏的)数据集合的内部特点/结构来确定它们的标签，这类模型虽然没有样本集的指示，但仍然依赖梯度下降法
  无监督学习常用于聚类任务
  - **聚类任务**：与分类任务相似，但聚类任务依据分析数据本身特点而非标签来分组，相比起来更简单
- 半监督学习指样本集中的部分样本点拥有标签，模型通过对有标签数据的学习后完成对剩下无标签数据贴标签的任务
  相比无监督学习，它通过消耗少部分人力，得到更好的预测效果
- **强化学习**的样本集同样没有标签，但它不倾向于分析样本集，而是**由奖励信号引导**，模型追求奖励最大化，最终得出全局的策略(似乎有点像动态规划)
- 评价模型的标准：
  - 泛化能力：指模型在经过样本集训练后，对未知数据集进行预测的能力
    预测结果越准确，称泛化误差越小，泛化能力强
  - 过拟合程度：模型对样本集的拟合效果较好，但泛化误差大，以至于显得像在对样本集死记硬背，称为过拟合
    原因：样本过少、样本噪音过大、模型过于复杂而敏感
  - 欠拟合程度：模型对样本集的拟合很差，导致泛化误差也很大，连给定的样本集都没学明白
    原因：模型过于简单而无法学习复杂的问题

## 传统机器学习

略

## 人工神经网络

### 基本概念

- 神经网络(`Neural Networks`)简写为**`NN`**
- 神经元：模拟人类大脑的神经元，由输入、权重和输出组成，是`NN`中最基本的模型
- `NN`的结构：
  - 分为**输入层**、**隐藏层**(除首层和尾层的所有层)、**输出层**，每层包含一些神经元
    经过隐藏层后，输出层中**激活值最大**的结点就是`NN`最后的选择
  - 模型参数：机器自己学习的权重、偏移量等参数，简称参数
  - 超参数：预先设定的层数、结点数、学习率等不变参数，就是人工调的参数
  - **全连接**：通常相邻层的结点都是两两相连的，一个结点和上层所有结点相连的连接方法称为全连接
    一个全连接层，即**该层的所有结点都分别和上一层的所有结点相连**

### 神经元的工作

输入经过每个神经元，需要进行**加权求和、添加偏置、激活函数**三步得到输出(并作为下一层的输入)

- 加权求和：每个输入对应一组权重，可将输入、权重表示为**列向量、矩阵**，分别记为$\boldsymbol x$和
  $w$；其中$w_{ij}$表示**本层第$i$个结点到下层第$j$个结点的权重**
  **到某个结点的加权求和**即为权重矩阵**某一列和输入向量的点积**
  一层结点的加权求和得到下一层输入的初步结果可记为$w^T\boldsymbol x$，是一种**线性变换**
  为了方便描述，此后将$\boldsymbol w_i$视作$w$的第$i$列，将$\boldsymbol w_i^T$视作$w$的第$i$列的转置
- 添加偏置(`bias`)：在**经过激活函数前**加上偏置，可以**控制**激活神经元的**阈值**，加速模型的拟合
  偏置会在加权求和后(即上一层结点**到下一层某一个结点$i$的交汇处**)加上，记为$b_i$，下一层所有结点的偏置也可以表示为列向量$\boldsymbol b$
  你可以想象它为本层的一个隐藏结点，该结点的输出固定为$1$，偏置即其到下一层结点的权重
- **激活函数**：加权求和后，需要通过激活函数来得到输出，由它完成从输入到输出(下一层的输入)的映射任务
  记为$g(x)$
  - 通常激活函数会**限制**输出的**值域**，控制均值
    如果一个结点的激活值为零(或接近零)，则它对下一层所有结点的贡献均为零(或很小)，形象地称该神经元死亡
  - 激活函数的必要性：输入和参数的初步运算(加权求和)是线性的
    通过使用非线性的激活函数，可使输出和输入的关系变为**非线性**，实现对非线性问题的拟合
  - 激活函数应该满足：**非线性**、连续**可导**、部署在隐藏层和输出层中
- 可以将神经元看作结点，用线表示权重、偏置及激活函数等，本层结点的输出为下层结点输入的一部分：
  <img src=".\pictures\NNStruct.png" alt="NNStruct" style="zoom:40%;" />
  (懒得重画了，手绘吧)
  以一个全连接层作为例子，输入$x_1^{(l)}$对应一组权重$[w_{11}^{(l)},w_{12}^{(l)},...,w_{1n}^{(l)}]$，$x_1^{(l+1)}$由上一层(和这个结点连接的)所有结点、$b_1^{(l)}$、$g^{(l+1)}(x)$共同决定
  第$l$层任意结点$x_i^{(l)}$对$x_1^{(l+1)}$的贡献可表示为$w_{i1}^{(l)}x_i^{(l)}$
  第$l$层所有结点对$x_1^{(l+1)}$的加权求和可表示为$\begin{align}\sum_{i\in l} w_{i1}^{(l)}x_i^{(l)}=\boldsymbol w_1^T\boldsymbol x^{(l)}\end{align}$
  第$l$层所有结点对第$l+1$层所有结点的加权求和可表示为一个列向量：$w^T\boldsymbol x^{(l)}$
  第$l$层所有结点对第$l+1$层的最终贡献即：$g^{(l+1)}(w^T\boldsymbol x^{(l)}+\boldsymbol b^{(l)})$，作为一个第$l$层向第$l+1$层的输出列向量，也同时作为第$l+1$层向第$l+2$层的输入列向量
  这就是后面将要提到的**前向传播过程**

### 学习过程

- **损失(代价)函数**
  - 一个尚未训练完成的`NN`，其权重、偏置等参数都不是最优的
    **所有样本**经过这样的参数后得到的答案集合会和标准答案有偏差
    这个偏差称为损失/代价(例如方差)，是**非负可测值**
    用于描述预测值和真实值间的差距大小
    损失越小，`NN`的准确度越高
    (损失指一个样本点的偏差，**代价指**整个样本集损失的平均，即**损失的期望**)
  - 损失函数是**从模型/算法映射到损失值**的函数，记为$L(y)$
- 学习特点：可测量的只有损失，训练`NN`或所谓学习的过程，就是使其**损失最小化**
  之所以说隐藏层是**黑盒**，是因为**不可解释**其每步预测的目的性(尽管最终选择是最优的)，尽管其内部结构是透明的，也无法解释其中数学计算的现实意义
- **初始化**输入层的输入、以及各结点的初始权重和偏置
- 进行多次的前向传播、反向传播、调整参数，调整参数的时机视使用的训练方法有关
- **前向传播过程**(`FP`)：递推计算所有层的输入，不再赘述
  - $x_i^{(l+1)}=g_i^{(l+1)}(\boldsymbol w_i^T\boldsymbol x^{(l)}+b_i^{(l)})$
  - $\boldsymbol x^{(l+1)}=\left[\begin{matrix}g_1^{(l+1)}(\boldsymbol w^T_1\boldsymbol x^{(l)}+b_1)\\\vdots\\g_n^{(l+1)}(\boldsymbol w^T_n\boldsymbol x^{(l)}+b_n)\end{matrix}\right]\xlongequal{g_i相同}g^{(l+1)}\left(w^T\boldsymbol x^{(l)}+\boldsymbol b^{(l)}\right)$
- **梯度下降法**：
  - 为了达成损失最小化，需要**使损失函数对所有参数的偏导为零**(达到极值)
    为了使该极值为极小值，需要向函数值减小的方向调整
    根据梯度的数学意义，**沿梯度的反方向调整**即可满足**导数下降**、**函数值下降**、**调整效果最大化**三个要求，故称梯度下降
  - 为了描述方便，假设第$l+1$层为输出层，即$x_i^{(l+1)}$就是模型最终的预测值
    先考察$L$对第$l$层参数的梯度，这一层的权重/偏置是特殊的(以权重为例)：
    因为任意一个$w_{ij}^{(l)}$只和$x_j^{(l+1)}$相关，即只和$L_j$相关
    于是$\begin{align}&\frac{\part L}{\part w_{ij}^{(l)}}=\frac{\part L_j}{\part w_{ij}^{(l)}}=\frac{\part L_j}{\part x_j^{(l+1)}}\frac{\part x_j^{(l+1)}}{\part w_{ij}^{(l)}}\end{align}$，左边即损失函数的导数$L'$
    由于$\begin{align}&x_j^{(l+1)}=g_j^{(l+1)}\left(\boldsymbol w_j^T\boldsymbol x^{(l)}+b_j^{(l)}\right)\end{align}$
    我们令$\begin{align}u_j^{(l)}=\boldsymbol w_j^T\boldsymbol x^{(l)}+b_j^{(l)}\end{align}$，即$\begin{align}\sum_{k\in l}w_{kj}x_k^{(l)}+b_j^{(l)}=w_{ij}x_i^{(l)}+C\end{align}$，$C$为和$w_{ij}^{(l)}$无关的常数项
    则$\begin{align}原式=L'\frac{\part g_j^{(l+1)}}{\part u_j^{(l)}}\frac{\part u_j^{(l)}}{\part w_{ij}^{(l)}}=L'\left(g_j^{(l+1)}\right)'x_i^{(l)}\end{align}$
    对于第$l$层的偏置，结果是类似的，只不过由于$\begin{align}\frac{\part x_j^{(l+1)}}{\part b_j^{(l)}}=1\end{align}$，梯度为$L'\left(g_j^{(l+1)}\right)'$
    再考察$L$对前$l-1$层参数的梯度，由于第$l+1$层为全连接层，第$l$层的输出的任意改动会影响所有$L_j$，自然而然地，前$l-1$层任意参数的梯度会涉及到所有$L_j$
    但求解并没有变得多复杂，因为总损失$\begin{align}L=\sum_{i\in l+1}L_i\end{align}$只是单纯的求和，所以(让我们以$w_{ij}^{(l-1)}$为例)$\begin{align}&\frac{\part L}{\part w_{ij}^{(l-1)}}=\sum_{k\in l+1}\frac{\part L_k}{\part w_{ij}^{(l-1)}}\end{align}$也是一个单纯的求和，其中每一项：
    $\begin{align}&\frac{\part L_k}{\part w_{ij}^{(l-1)}}=\frac{\part L_k}{\part x_{j}^{(l)}}·\frac{\part x_j^{(l)}}{\part w_{ij}^{(l-1)}}=\frac{\part L_k}{\part x_{k}^{(l+1)}}\frac{\part x_k^{(l+1)}}{\part u_k^{(l)}}\frac{\part u_k^{(l)}}{\part x_j^{(l)}}·\frac{\part x_j^{(l)}}{\part u_j^{(l-1)}}\frac{\part u_j^{(l-1)}}{\part w_{ij}^{(l-1)}}\\&=L'\left(g_{k}^{(l+1)}\right)'w_{jk}^{(l)}·\left(g_j^{(l)}\right)'x_i^{(l-1)}\end{align}$
    于是$\begin{align}&原式=L'\left(\sum_{k\in l+1}\left(g_k^{(l+1)}\right)'w_{jk}^{(l)}\right)\left(g_j^{(l)}\right)'x_i^{(l-1)}\end{align}$
    过程是很繁琐的，但熟悉链式法则的话，展开是很顺畅的
  - 简化：继续这么向前求下去，一定会求和套求和，你可以求一下$w_{ij}^{(l-2)}$的梯度
    我们可以将这种求和表示为向量的点积，并将式子写成迭代的形式，这样会简洁很多
    (这也是网上一堆所谓简单易懂教程的做法，事实上一上来就简化就是很难懂)
    令$\begin{align}\delta^{(k)}_i=\begin{cases}L'\left(g_i^{(l+1)}\right)',&k=l+1\\ [w_{i1}^{(k)},w_{i2}^{(k)},\cdots,w_{in}^{(k)}]·\boldsymbol\delta^{(k+1)}·\left(g_i^{(k)}\right)',&k<l+1\end{cases}\end{align}$
    简单来说就是，$\boldsymbol\delta^{(k)}$是一个列向量
    输出层的$\boldsymbol\delta$每一项都是$L'\left(g_i^{(l+1)}\right)'$
    往前所有层的$\boldsymbol\delta$的第$i$项是该层权重矩阵的**第$i$行**和后一层$\boldsymbol\delta$的点积再乘上该层第$i$个激活函数的导数
    有了这个，前面求的梯度都可以简化，例如：
    $\begin{align}&\frac{\part L}{\part w_{ij}^{(l)}}=\delta_j^{(l+1)}x_i^{(l)}\\&\frac{\part L}{\part b_j^{(l)}}=\delta_j^{(l+1)}\\&\frac{\part L}{\part w_{ij}^{(l-1)}}=\delta_j^{(l)}x_i^{(l-1)}\end{align}$
    你可以展开来验证一下，是否和之前算的一致
- **反向传播过程**(`BP`)：根据计算得到的梯度，更新所有层的权重矩阵和偏置
  - 计算所有结点的$\delta$：还是以全连接层为例，假设选用的损失函数为$L(y)$，预测值为$\hat{\boldsymbol y}$，实际值为$\boldsymbol y$，则输出层产生的$\delta$为$\begin{align}L'(\boldsymbol y-\hat{\boldsymbol{y}})\left(\boldsymbol g^{(l+1)}\right)'\end{align}$，记为列向量$\boldsymbol \delta^{(l+1)}$
    接着根据递推式往前一层传播并计算前一层结点的$\delta$：
    $\boldsymbol\delta^{(l)}=w^{(l)}\boldsymbol\delta^{(l+1)}·\left(g^{(l)}\right)'$
  - 一旦有了一层的$\delta$后，先递推得到前一层的$\delta$再更新前一层的权重矩阵及偏置(递推依赖于原有的参数)
    如梯度下降法推导的那样，任意权重梯度为$\begin{align}&\frac{\part L}{\part w_{ij}^{(l)}}=\delta_j^{(l+1)}x_i^{(l)}\end{align}$、任意偏置梯度为$\begin{align}\frac{\part L}{\part b_j^{(l)}}=\delta_j^{(l+1)}\end{align}$
    沿其负方向更改，即$w_{ij}^{(l)}\leftarrow w_{ij}^{(l)}-\eta\delta_j^{(l+1)}x_i^{(l)}$
    其中$\eta$是学习率，它是一个超参数，模型不会自动更新它
    可以将梯度表示为矩阵的形式：$w^{(l)}\leftarrow w^{(l)}-\eta\ \boldsymbol x^{(l)}\left(\boldsymbol\delta^{(l+1)}\right)^T$
    偏置的更新与其类似，反向传播直到$l=1$
    需要说明的是，这个**参数更新的时机因反向传播算法的不同而不同**
- 常用反向传播算法：
  没有说哪个更好，有缺陷就有对应的解决方案，这里的介绍只是它们最原始的样子
  - 随机梯度下降法(`SGD`)/增量梯度下降法(`IGD`)：随机选择一个样本点训练，每处理一个样本点，立刻根据梯度更新参数
    - 训练很快，能确保在局部最优解附近
    - 像一个醉鬼一样下山，走到哪算哪，可能无法走到全局最优解(损失函数有多个极小值点，可能只能走到较大的极小值点而无法走到最小值点)
  - 批量梯度下降法(`BGD`)：批量即每次求损失值时，计算所有样本的梯度并存储(暂时不更新)
    在处理完整个样本集后，对梯度求平均后更新
    - 每次迭代时能稳定地指向最准确的极值方向，结果是全局最优的
    - 当样本特别多时，一次的训练时间过长
  - 小批量梯度下降(`MBGD`)：小批量如其名，即将样本集划分为多个小样本集，一次训练对小样本集进行上述操作
    是一种折中的方法，兼顾训练效率、模型准确度
- 到这里应该能理解，为什么泛化能力是评价模型的重要指标，为什么会出现过拟合、次拟合问题
  通过训练集有限的样本点计算出来的梯度，始终无法代表模型在真实环境下经过无穷数据产生的损失
  因为不可避免地会有模型处理完训练集后，落在局部最优解，而这个局部最优解离全局最优解还很远的情况
  因此有了验证集、测试集的概念，来评估模型的泛化能力、过拟合程度等
  - 验证集：用于初步评估模型的泛化能力，不直接调整模型参数，但会**调整超参数**
    每过一个训练周期，进行一次对验证集的计算
    当然，模型可以对训练集过拟合，肯定也有对验证集过拟合的情况
  - 测试集：模型定型后，用于检查它的最终泛化能力，必须保证计算过程不进行任何对模型的调整
    必须是大量的、能反映真实环境复杂性的样本集
    否则，模型偷偷摸摸地背书，你还写在论文上有多高多高的准确率，那就是学术不端了
    在模型任何参数不变、输入的样本集也不变的情况下，结果是一样的，是可以完全复现的


### 常见神经网络

## 常见激活函数

- `Sigmoid`(又称`Logistic`)：$\begin{align}\sigma(x)=\frac1{1+e^{-x}}、\sigma'(x)=\frac{e^{-x}}{(1+e^{-x})^2}=\sigma(x)(1-\sigma(x))\end{align}$
  - 定义域为$(-\infty,+\infty)$，值域为$(0,1)$，是一种二分类函数
  - 性质：严格单调递增、输出范围有界且狭小
  - 缺点：
    - 输入的绝对值较大时，输出达到饱和，对输入变化不敏感，容易丢失这部分的梯度信息
    - 输出恒大于零，意味着均值不为零，将导致后续神经元的输入也是非零均值的
- `Tanh`(双曲正切)：$\begin{align}\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}=2\sigma(x)-1\end{align}$
  - 定义域为$(-\infty,+\infty)$，值域为$(-1,1)$
  - 解决了`Sigmoid`函数非零均值的问题
- `ReLU`(`Rectified linear unit`，整流线性单元)：$f(x)=\max(0,x)、f'(x)=\begin{cases}1,&x>0\\0,&x\le0\end{cases}$
  - 是最常用的激活函数
  - 性质：计算简单、求导简单、正输入的输出不会饱和
  - 缺点：
    - 非零均值
    - `Dead ReLU`(神经元坏死)：负输入的输出为零，容易导致死一大片神经元
- `Leaky ReLU`：$\begin{align}&f(x)=\max(\alpha x,x),&\alpha\text{为十分小的固定正数}\end{align}$
  `PReLU`：$\begin{align}&f(x)=\max(\alpha x,x),&\alpha\text{为十分小的正数,可在学习过程中改变}\end{align}$
  - 负输入的输出将向下渗漏，不恒为零
  - 前者的参数是固定的，使函数近似线性，后者的$\alpha$则为超参数
- `ELU`：$f(x)=\begin{cases}x,&x>0\\\alpha(e^x-1),&x\le0\end{cases}$
- `Maxout`：

## 常见损失函数

- 为使用梯度下降法，损失函数应当为下凸函数，使代价能下降到全局最小值
  根据模型进行分类/回归/聚类等不同任务，应采取不同的损失函数
- 分类问题：
  - 交叉熵：
  - `Hinge`：
  - 余弦相似度：
  - 指数：
- 回归问题：
  - `MSE`：
  - `MAE`：
  - `Huber`：
  - `Log-cosh`：
  - 分位数：